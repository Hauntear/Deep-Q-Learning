{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform as skim\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #Convolutional neural network.\n",
    "        self.scalarInput = tf.placeholder(shape=[None,28224],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape=[-1,84,84,4])\n",
    "        self.conv1 = slim.conv2d(inputs=self.imageIn,num_outputs=16,kernel_size=[8,8],stride=[4,4],\n",
    "                                padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d(inputs=self.conv1,num_outputs=32,kernel_size=[6,6],stride=[3,3],\n",
    "                                padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d(inputs=self.conv2,num_outputs=64,kernel_size=[4,4],stride=[1,1],\n",
    "                                padding='VALID', biases_initializer=None)\n",
    "        #self.conv4 = slim.conv2d(inputs=self.conv3,num_outputs=h_size,kernel_size=[5,5],stride=[2,2],\n",
    "                                #padding='VALID', biases_initializer=None)\n",
    "        \n",
    "        #Output of conv3 into value streams.\n",
    "        self.res = self.conv3\n",
    "        self.streamA = slim.flatten(self.res)\n",
    "        self.streamV = slim.flatten(self.res)\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        self.AW = tf.Variable(xavier_init([h_size,4]))\n",
    "        self.VW = tf.Variable(xavier_init([h_size,1]))\n",
    "        #Compute Advantage value of state.\n",
    "        self.Advantage = tf.matmul(self.streamA,self.AW)\n",
    "        #Compute Q-values of actions.\n",
    "        self.Value = tf.matmul(self.streamV,self.VW)\n",
    "        \n",
    "        #Combine for final Q-values.\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keepdims=True))\n",
    "        #Compute action with greatest Q-value.\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Compute loss through sum squares difference of prediction Q-values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions,4,dtype=tf.float32)\n",
    "        \n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        self.loss = tf.reduce_mean(self.td_error)\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.00025)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class for handling experience gained.\n",
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 12500):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self, experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "\n",
    "    def sample(self, size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocess state obeservations by converting to grey-scale, resizing then cropping for relevant game info.\n",
    "def processState(states):\n",
    "    s = np.dot(states[...,:3], [0.299, 0.587, 0.114])\n",
    "    s = skim.resize(s,[110,84],mode='constant')\n",
    "    s = s[26:110,0:84]\n",
    "    return np.reshape(s,[1,7056])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Slow update of target network parameters to minimize main Q-network Q-value overestimates.\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+\n",
    "                        total_vars//2].value())))\n",
    "        return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = 0.99 #Discount factior on the target Q-values.\n",
    "startE = 1 #starting chance of random action.\n",
    "endE = 0.1 #Final chance of random action.\n",
    "annealing_steps = 1000000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 10000000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #The many steps of random actions before training begins.\n",
    "load_model = False #Whether to load a model or not.\n",
    "path = './4dqn' #The path to save model to.\n",
    "h_size = 256 #The size of the final convolutional layer before splitting.\n",
    "tau = 0.001 #Rate of update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "save = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "sf = np.zeros([1,7056])\n",
    "\n",
    "#Set the rate of random action decrease.\n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#Create lists to contain total reward and steps per episode.\n",
    "jList = []\n",
    "rList = []\n",
    "\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "#Training the Q-network.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #Load latest saved model in path if load_model==True.\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        save.restore(sess,ckpt.model_checkpoint_path)\n",
    "    for i in range(num_episodes):\n",
    "        internal_buffer = [sf,sf,sf]\n",
    "        episodeBuffer = experience_buffer()\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        internal_buffer.append(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #Episode.\n",
    "        while not d:\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(4)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.hstack(internal_buffer[-4:])})[0]\n",
    "            #Execute action a and recieve new state observation, reward\n",
    "            s1,r,d,info = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            internal_buffer.append(s1)\n",
    "            total_steps += 1\n",
    "            episodeBuffer.add(np.reshape(np.array([np.hstack(internal_buffer[-5:-1]),a,r,\n",
    "                                                   np.hstack(internal_buffer[-4:]),d]),[1,5]))\n",
    "            #Decrease e linearly over training when pre-train steps are over.\n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                #Update main Q-network and target Q-network every fourth step.\n",
    "                if total_steps % update_freq == 0:\n",
    "                    trainBatch = myBuffer.sample(batch_size)\n",
    "                    Q1 = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    Q2 = sess.run(targetQN.Qout,feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3])})\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    doubleQ = Q2[range(batch_size),Q1]\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ * end_multiplier)\n",
    "                    _ = sess.run(mainQN.updateModel, feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),\n",
    "                                                                   mainQN.targetQ:targetQ, mainQN.actions:trainBatch[:,1]})\n",
    "                    updateTarget(targetOps,sess)\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            print('-------------------------------------------------')\n",
    "            print(i,np.mean(rList[-1000:]))\n",
    "            save.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(i,np.mean(rList[-10:]), e)\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evalQN = Qnetwork(h_size)\n",
    "evalBatchSize = 10000\n",
    "env = gym.make('Breakout-v0')\n",
    "\n",
    "save = tf.train.Saver()\n",
    "sf = np.zeros([1,7056])\n",
    "\n",
    "evalrList = []\n",
    "\n",
    "\n",
    "#Tests the latest saved policy in path over a sample of 10000 games.\n",
    "with tf.Session() as sess:\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    print('Loading Model', ckpt.model_checkpoint_path + '...')\n",
    "    save.restore(sess,ckpt.model_checkpoint_path)\n",
    "    for i in range(10000):\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        eval_frame_buffer = [sf,sf,sf]\n",
    "        eval_frame_buffer.append(s)\n",
    "        d = False\n",
    "        evalrAll = 0\n",
    "        while not d:\n",
    "            a = sess.run([predict], feed_dict={evalQN.scalaInput:np.hstack(eval_fram_buffer[-4:])})\n",
    "            s1, r, d, info = env.step(a)\n",
    "            s1 = processState(s1)\n",
    "            eval_frame_buffer.append(s1)\n",
    "            evalrAll += r\n",
    "            if d:\n",
    "                break\n",
    "\n",
    "rMat = np.resize(np.array(evalrList),[len(evalrList)//100,100])\n",
    "rMean = np.average(rMat,1)\n",
    "plt.plot(rMean)\n",
    "                \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
